# InfiniLM 多模态模型与 MLA 模块实现中期报告

## 一、项目概述

本报告涵盖两个核心任务的开发进展：

1. **多模态模型实现（Qwen3-VL-2B-Instruct）** - 困难任务，奖金 ￥14,000
2. **MLA 模块实现（DeepSeek-R1）** - 困难任务，奖金 ￥6,000

### 1.1 任务目标

#### 任务 2-1-4：多模态模型
- **模型**：Qwen3-VL-2B-Instruct
- **模型链接**：https://huggingface.co/Qwen/Qwen3-VL-2B-Instruct
- **必需平台**：英伟达
- **可选平台**：摩尔、天数、沐曦
- **核心要求**：实现 Vision Encoder 等模型结构以及相应的算子

#### 任务 2-2-3：MLA 模块
- **模型来源**：DeepSeek-R1
- **模型链接**：https://huggingface.co/deepseek-ai/DeepSeek-R1
- **模型格式**：BF16 格式（已反量化）
- **必需平台**：英伟达
- **可选平台**：摩尔、天数、沐曦
- **核心要求**：实现 MLA（Multi-head Latent Attention）模块，支持小批量预填充和大批量解码场景

## 二、环境搭建与准备工作

### 2.1 开发环境配置

#### 2.1.1 基础环境
- **操作系统**：Ubuntu 24.04.3 LTS (WSL2)
- **编译器**：GCC 13.3.0
- **构建工具**：Xmake v3.0.5
- **Python 环境**：Miniconda3 (Python 3.13.11)

#### 2.1.2 CUDA 环境
- **CUDA 版本**：CUDA 13.1
- **GPU**：NVIDIA GeForce RTX 5070 Ti (Compute Capability 12.0)
- **CUDA 路径**：/usr/local/cuda

#### 2.1.3 项目依赖
- **InfiniCore**：已成功编译安装（CPU + GPU 版本）
- **安装路径**：~/.infini
- **核心库**：
  - libinfiniop.so
  - libinfinirt.so
  - libinfiniccl.so
  - libinfinicore_cpp_api.so

### 2.2 项目结构分析

已完成对 InfiniLM 项目结构的深入分析：
- 模型实现位于 `src/models/` 目录
- 测试脚本位于 `test/models/` 目录
- 参考实现：Qwen3-MoE 的 attention_test.py 作为测试脚本模板

## 三、任务进展

### 3.1 多模态模型（Qwen3-VL-2B-Instruct）实现进展

#### 3.1.1 已完成工作

1. **模型结构分析**
   - 深入研究了 Qwen3-VL-2B-Instruct 的架构
   - 分析了 Vision Encoder 的结构和实现需求
   - 识别了需要实现的核心算子

2. **Vision Encoder 设计**
   - 确定了 Vision Encoder 的输入输出格式
   - 设计了图像预处理流程
   - 规划了 Vision Encoder 与语言模型的融合方案

3. **算子实现规划**
   - 识别了 Vision Encoder 所需的关键算子
   - 分析了这些算子在 InfiniCore 框架下的实现方式
   - 制定了算子实现优先级

#### 3.1.2 技术方案

- **Vision Encoder 架构**：基于 Transformer 的图像编码器
- **图像预处理**：支持多分辨率图像输入
- **特征融合**：Vision 特征与文本 token 的融合机制
- **算子优化**：针对英伟达 GPU 的算子优化策略

#### 3.1.3 当前状态

- [x] 环境搭建完成
- [x] 模型结构分析完成
- [ ] Vision Encoder 实现（进行中）
- [ ] 相关算子实现（待开始）
- [ ] 测试脚本编写（待开始）

### 3.2 MLA 模块（DeepSeek-R1）实现进展

#### 3.2.1 已完成工作

1. **MLA 模块分析**
   - 深入研究了 DeepSeek-R1 的 MLA 模块实现
   - 参考实现：https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/model.py
   - 理解了 "kv_cache + pe_cache" 缓存策略

2. **SageAttention 集成**
   - 确定使用 SageAttention 作为核心注意力机制
   - 分析了 SageAttention 在 MLA 模块中的应用方式
   - 设计了缓存管理策略

3. **性能测试场景设计**
   - **小批量预填充场景**：
     - 4 个预填充请求，长度分别为 64、128、256、256
     - 历史长度分别为 512、0、0、256
     - 随机初始化输入和 KV Cache
     - 100 轮推理，计算平均延迟
   
   - **大批量解码场景**：
     - 16 个解码请求，输入长度均为 1
     - 历史长度为 50×4、100×4、200×4、400×4
     - 顺序推理 100 轮，计算生成一个 token 的平均延迟

#### 3.2.2 技术方案

- **注意力机制**：基于 SageAttention 的 MLA 实现
- **缓存策略**：kv_cache + pe_cache 双缓存机制
- **批量化处理**：支持动态批量化，优化性能
- **数据类型**：BF16 格式支持（无需 FP8）

#### 3.2.3 当前状态

- [x] 环境搭建完成
- [x] MLA 模块结构分析完成
- [x] SageAttention 集成方案确定
- [ ] MLA 模块实现（进行中）
- [ ] 测试脚本编写（待开始）
- [ ] 性能优化（待开始）

## 四、技术难点与解决方案

### 4.1 多模态模型实现难点

#### 难点 1：Vision Encoder 与语言模型的融合
- **问题**：如何高效地将图像特征与文本 token 融合
- **解决方案**：设计统一的多模态 token 表示，优化特征对齐机制

#### 难点 2：多分辨率图像处理
- **问题**：不同尺寸图像的批处理
- **解决方案**：实现动态 padding 和 attention mask 机制

#### 难点 3：算子性能优化
- **问题**：Vision Encoder 相关算子在 GPU 上的性能优化
- **解决方案**：利用 CUDA kernel 优化，参考 InfiniCore 现有算子实现

### 4.2 MLA 模块实现难点

#### 难点 1：SageAttention 集成
- **问题**：如何将 SageAttention 正确集成到 MLA 模块中
- **解决方案**：深入理解 SageAttention 的 API 和缓存机制，设计适配层

#### 难点 2：双缓存策略实现
- **问题**：kv_cache + pe_cache 的同步管理
- **解决方案**：设计统一的缓存管理器，确保两个缓存的正确更新和同步

#### 难点 3：动态批量化
- **问题**：不同历史长度的请求如何高效批处理
- **解决方案**：实现智能批分组策略，平衡计算效率和内存使用

#### 难点 4：性能测试场景实现
- **问题**：小批量预填充和大批量解码的性能测试
- **解决方案**：参考现有测试脚本，设计专门的性能测试框架

## 五、下一步工作计划

### 5.1 多模态模型（优先级：高）

**Week 1-2：Vision Encoder 核心实现**
- [ ] 实现 Vision Encoder 的基础结构
- [ ] 实现图像预处理算子
- [ ] 实现 Vision Encoder 的前向传播

**Week 3-4：算子实现与优化**
- [ ] 实现 Vision Encoder 所需的关键算子
- [ ] GPU 算子优化
- [ ] 与语言模型的融合接口实现

**Week 5-6：测试与优化**
- [ ] 编写测试脚本
- [ ] 正确性验证
- [ ] 性能优化

### 5.2 MLA 模块（优先级：高）

**Week 1-2：MLA 模块核心实现**
- [ ] 基于 SageAttention 实现 MLA 模块
- [ ] 实现 kv_cache + pe_cache 双缓存机制
- [ ] 实现动态批量化逻辑

**Week 3：测试脚本编写**
- [ ] 参考 qwen3_moe/attention_test.py 编写测试脚本
- [ ] 实现正确性测试（与第三方框架对比）
- [ ] 实现性能测试（小批量预填充 + 大批量解码）

**Week 4：性能优化与验证**
- [ ] 性能瓶颈分析
- [ ] GPU 算子优化
- [ ] 性能测试验证

### 5.3 可选平台支持（优先级：中）

- [ ] 摩尔线程 GPU 支持
- [ ] 天数智芯 GPU 支持
- [ ] 沐曦 GPU 支持

## 六、遇到的问题与解决方案

### 6.1 环境搭建问题

**问题 1**：xmake 版本过旧，不支持 `python.module` 规则
- **解决方案**：更新 xmake 到 v3.0.5，卸载旧版本

**问题 2**：缺少 Python 开发头文件
- **解决方案**：安装 Miniconda，使用 conda 环境中的 Python

**问题 3**：缺少 CUDA 和 cuDNN
- **解决方案**：安装 CUDA 13.1 Toolkit（cuDNN 待安装或禁用）

### 6.2 技术实现问题

**问题**：SageAttention 的集成方式需要进一步研究
- **状态**：正在研究中
- **计划**：深入阅读 SageAttention 文档和源码

## 七、总结

### 7.1 当前进度

- **环境搭建**：✅ 100% 完成
- **多模态模型**：🔄 30% 完成（结构分析完成，实现进行中）
- **MLA 模块**：🔄 40% 完成（方案确定，实现进行中）

### 7.2 预期完成时间

- **MLA 模块**：预计 4 周内完成
- **多模态模型**：预计 6 周内完成

### 7.3 风险与应对

**风险 1**：SageAttention 集成复杂度超出预期
- **应对**：提前深入研究，必要时寻求社区支持

**风险 2**：性能优化难度大
- **应对**：参考 InfiniCore 现有优化经验，分阶段优化

**风险 3**：测试场景实现复杂
- **应对**：参考现有测试脚本，逐步实现

---

**报告日期**：2025年12月26日  
**项目状态**：进行中  
**下一步重点**：完成 MLA 模块核心实现和 Vision Encoder 基础结构

